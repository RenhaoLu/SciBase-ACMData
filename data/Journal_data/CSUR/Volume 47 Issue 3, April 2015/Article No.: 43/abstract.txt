Affect detection is an important pattern recognition problem that has inspired researchers from several areas. The field is in need of a systematic review due to the recent influx of Multimodal (MM) affect detection systems that differ in several respects and sometimes yield incompatible results. This article provides such a survey via a quantitative review and meta-analysis of 90 peer-reviewed MM systems. The review indicated that the state of the art mainly consists of person-dependent models (62.2&percnt; of systems) that fuse audio and visual (55.6&percnt;) information to detect acted (52.2&percnt;) expressions of basic emotions and simple dimensions of arousal and valence (64.5&percnt;) with feature- (38.9&percnt;) and decision-level (35.6&percnt;) fusion techniques. However, there were also person-independent systems that considered additional modalities to detect nonbasic emotions and complex dimensions using model-level fusion techniques. The meta-analysis revealed that MM systems were consistently (85&percnt; of systems) more accurate than their best unimodal counterparts, with an average improvement of 9.83&percnt; (median of 6.60&percnt;). However, improvements were three times lower when systems were trained on natural (4.59&percnt;) versus acted data (12.7&percnt;). Importantly, MM accuracy could be accurately predicted (cross-validatedR2of 0.803) from unimodal accuracies and two system-level factors. Theoretical and applied implications and recommendations are discussed.